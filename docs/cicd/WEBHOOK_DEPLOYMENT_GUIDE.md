# GitHub Webhook ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ»é‹ç”¨ã‚¬ã‚¤ãƒ‰

**ã‚¨ã‚¹ãƒ»ã‚¨ãƒ¼ãƒ»ã‚¨ã‚¹æ ªå¼ä¼šç¤¾**  
*GitHub Webhook ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ»é‹ç”¨ãƒãƒ‹ãƒ¥ã‚¢ãƒ«*

## ğŸ“‹ ç›®æ¬¡

1. [ãƒ‡ãƒ—ãƒ­ã‚¤æ¦‚è¦](#ãƒ‡ãƒ—ãƒ­ã‚¤æ¦‚è¦)
2. [ç’°å¢ƒæ§‹æˆ](#ç’°å¢ƒæ§‹æˆ)
3. [å‰ææ¡ä»¶](#å‰ææ¡ä»¶)
4. [ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †](#ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †)
5. [è¨­å®šç®¡ç†](#è¨­å®šç®¡ç†)
6. [ç›£è¦–ãƒ»ãƒ­ã‚°](#ç›£è¦–ãƒ­ã‚°)
7. [é‹ç”¨æ‰‹é †](#é‹ç”¨æ‰‹é †)
8. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)
9. [ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é‹ç”¨](#ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é‹ç”¨)
10. [ç½å®³å¾©æ—§](#ç½å®³å¾©æ—§)

## ğŸ“Œ ãƒ‡ãƒ—ãƒ­ã‚¤æ¦‚è¦

### ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ

```mermaid
graph TB
    subgraph "GitHub"
        GH[GitHub.com]
    end
    
    subgraph "Load Balancer"
        LB[AWS ALB / Azure LB]
    end
    
    subgraph "Kubernetes Cluster"
        subgraph "Webhook Services"
            NS[Node.js Service]
            PS[Python Service]
            GS[Go Service]
        end
        
        subgraph "Infrastructure"
            REDIS[Redis Cache]
            DB[(PostgreSQL)]
            ES[Elasticsearch]
        end
        
        subgraph "Monitoring"
            PROM[Prometheus]
            GRAF[Grafana]
            ALERT[AlertManager]
        end
    end
    
    subgraph "External Services"
        SLACK[Slack/Teamsé€šçŸ¥]
        EMAIL[ãƒ¡ãƒ¼ãƒ«é€šçŸ¥]
        SIEM[SIEM/SOC]
    end
    
    GH --> LB
    LB --> NS
    LB --> PS
    LB --> GS
    
    NS --> REDIS
    PS --> REDIS
    GS --> REDIS
    
    NS --> DB
    PS --> DB
    GS --> DB
    
    NS --> ES
    PS --> ES
    GS --> ES
    
    PROM --> GRAF
    PROM --> ALERT
    
    ALERT --> SLACK
    ALERT --> EMAIL
    ALERT --> SIEM
```

### å¯¾å¿œãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 

| ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  | ã‚µãƒãƒ¼ãƒˆçŠ¶æ³ | æ¨å¥¨åº¦ |
|-----------------|-------------|--------|
| **AWS EKS** | âœ… å®Œå…¨å¯¾å¿œ | â˜…â˜…â˜… |
| **Azure AKS** | âœ… å®Œå…¨å¯¾å¿œ | â˜…â˜…â˜… |
| **Google GKE** | âœ… å®Œå…¨å¯¾å¿œ | â˜…â˜…â˜… |
| **ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹** | âš ï¸ åˆ¶é™ä»˜ã | â˜…â˜†â˜† |
| **Docker Compose** | âœ… é–‹ç™ºç”¨ | â˜…â˜…â˜† |

## ğŸ—ï¸ ç’°å¢ƒæ§‹æˆ

### æ¨å¥¨ç’°å¢ƒæ§‹æˆ

#### æœ¬ç•ªç’°å¢ƒ (Production)
```yaml
environments:
  production:
    cluster:
      nodes: 6-12
      instance_type: "m5.xlarge" # AWS
      cpu: "4 vCPU"
      memory: "16GB RAM"
      storage: "100GB SSD"
    
    webhook_services:
      replicas: 3
      resources:
        requests:
          cpu: "500m"
          memory: "1Gi"
        limits:
          cpu: "2000m"
          memory: "4Gi"
    
    database:
      type: "Managed Service"
      instance: "db.r5.large"
      storage: "500GB"
      backup_retention: "30 days"
      multi_az: true
    
    cache:
      type: "ElastiCache/Azure Redis"
      instance: "cache.m5.large"
      nodes: 3
      
    monitoring:
      retention: "1 year"
      metrics_interval: "15s"
      log_retention: "90 days"
```

#### ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒ (Staging)
```yaml
environments:
  staging:
    cluster:
      nodes: 3
      instance_type: "m5.large"
      cpu: "2 vCPU"
      memory: "8GB RAM"
      storage: "50GB SSD"
    
    webhook_services:
      replicas: 2
      resources:
        requests:
          cpu: "250m"
          memory: "512Mi"
        limits:
          cpu: "1000m"
          memory: "2Gi"
```

#### é–‹ç™ºç’°å¢ƒ (Development)
```yaml
environments:
  development:
    cluster:
      nodes: 1
      instance_type: "m5.medium"
      cpu: "1 vCPU"
      memory: "4GB RAM"
      storage: "20GB SSD"
    
    webhook_services:
      replicas: 1
      resources:
        requests:
          cpu: "100m"
          memory: "256Mi"
        limits:
          cpu: "500m"
          memory: "1Gi"
```

## ğŸ”§ å‰ææ¡ä»¶

### ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶

#### Kubernetes ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
- **Kubernetes ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.25ä»¥ä¸Š
- **Container Runtime**: containerd ã¾ãŸã¯ CRI-O
- **Network Plugin**: Calico, Weave Net, ã¾ãŸã¯ AWS VPC CNI
- **Ingress Controller**: NGINXã€ALBã€ã¾ãŸã¯ Traefik
- **Certificate Manager**: cert-manager
- **DNS**: CoreDNS

#### å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹
- **GitHub Enterprise Cloud**: Webhookè¨­å®šæ¨©é™
- **DNSç®¡ç†**: Route53ã€CloudDNSã€ã¾ãŸã¯ Azure DNS
- **SSLè¨¼æ˜æ›¸**: Let's Encrypt ã¾ãŸã¯ä¼æ¥­è¨¼æ˜æ›¸
- **ç§˜å¯†ç®¡ç†**: AWS Secrets Managerã€Azure Key Vaultã€ã¾ãŸã¯ HashiCorp Vault

### å¿…è¦ãªãƒ„ãƒ¼ãƒ«

#### ç®¡ç†ãƒ„ãƒ¼ãƒ«
```bash
# Kubernetesç®¡ç†
kubectl >= 1.25
helm >= 3.8

# ã‚¯ãƒ©ã‚¦ãƒ‰ CLI
aws-cli >= 2.0      # AWSä½¿ç”¨æ™‚
az-cli >= 2.40      # Azureä½¿ç”¨æ™‚
gcloud >= 400.0     # GCPä½¿ç”¨æ™‚

# ç›£è¦–ãƒ»ãƒ­ã‚°
prometheus-operator
grafana
elasticsearch-operator

# CI/CD
github-actions      # GitHub Actions
argocd             # ArgoCD (æ¨å¥¨)
fluxcd             # Flux CD (ä»£æ›¿)
```

#### é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«
```bash
# ã‚³ãƒ³ãƒ†ãƒŠ
docker >= 20.10
docker-compose >= 2.0

# ãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼
curl
jq
httpie
k6                 # è² è·ãƒ†ã‚¹ãƒˆ

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
trivy              # è„†å¼±æ€§ã‚¹ã‚­ãƒ£ãƒ³
cosign             # ã‚³ãƒ³ãƒ†ãƒŠç½²å
falco              # ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
```

## ğŸš€ ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †

### Step 1: ç’°å¢ƒæº–å‚™

#### 1.1 Kubernetes ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½œæˆ

**AWS EKS**:
```bash
# EKS ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½œæˆ
eksctl create cluster \
  --name sas-webhook-cluster \
  --version 1.27 \
  --region ap-northeast-1 \
  --nodegroup-name webhook-nodes \
  --node-type m5.xlarge \
  --nodes 3 \
  --nodes-min 3 \
  --nodes-max 12 \
  --ssh-access \
  --ssh-public-key sas-webhook-key \
  --managed

# EKS cluster ã¸ã®æ¥ç¶šè¨­å®š
aws eks update-kubeconfig --region ap-northeast-1 --name sas-webhook-cluster
```

**Azure AKS**:
```bash
# AKS ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½œæˆ
az aks create \
  --resource-group sas-webhook-rg \
  --name sas-webhook-cluster \
  --kubernetes-version 1.27.3 \
  --location japaneast \
  --node-count 3 \
  --node-vm-size Standard_D4s_v3 \
  --enable-managed-identity \
  --enable-addons monitoring \
  --generate-ssh-keys

# AKS cluster ã¸ã®æ¥ç¶šè¨­å®š
az aks get-credentials --resource-group sas-webhook-rg --name sas-webhook-cluster
```

**Google GKE**:
```bash
# GKE ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½œæˆ
gcloud container clusters create sas-webhook-cluster \
  --zone asia-northeast1-a \
  --num-nodes 3 \
  --machine-type e2-standard-4 \
  --cluster-version 1.27 \
  --enable-autorepair \
  --enable-autoupgrade \
  --enable-network-policy

# GKE cluster ã¸ã®æ¥ç¶šè¨­å®š
gcloud container clusters get-credentials sas-webhook-cluster --zone asia-northeast1-a
```

#### 1.2 å¿…é ˆã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# Helmè¿½åŠ 
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo add elastic https://helm.elastic.co
helm repo update

# Ingress Controller
helm install nginx-ingress ingress-nginx/ingress-nginx \
  --namespace ingress-nginx \
  --create-namespace \
  --set controller.replicaCount=3 \
  --set controller.nodeSelector."kubernetes\.io/arch"=amd64

# cert-manager (SSLè¨¼æ˜æ›¸ç®¡ç†)
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml

# Prometheus Operator
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set prometheusOperator.enabled=true \
  --set prometheus.enabled=true \
  --set alertmanager.enabled=true \
  --set grafana.enabled=true
```

### Step 2: ç§˜å¯†ç®¡ç†ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### 2.1 Kubernetes Secretsä½œæˆ

```bash
# Webhookç§˜å¯†éµ (GitHubè¨­å®šã¨åŒã˜å€¤ã‚’ä½¿ç”¨)
kubectl create secret generic webhook-secrets \
  --namespace webhook \
  --from-literal=WEBHOOK_SECRET="your-super-secure-webhook-secret" \
  --from-literal=DATABASE_URL="postgresql://user:password@db-host:5432/webhooks" \
  --from-literal=REDIS_URL="redis://redis-host:6379" \
  --from-literal=ELASTICSEARCH_URL="https://es-host:9200"

# TLSè¨¼æ˜æ›¸ (Let's Encryptã¾ãŸã¯ä¼æ¥­è¨¼æ˜æ›¸)
kubectl create secret tls webhook-tls \
  --namespace webhook \
  --cert=webhook.sas-com.com.crt \
  --key=webhook.sas-com.com.key
```

#### 2.2 å¤–éƒ¨ç§˜å¯†ç®¡ç†çµ±åˆ (æ¨å¥¨)

**AWS Secrets Managerçµ±åˆ**:
```yaml
# external-secrets-operator
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: aws-secrets-manager
  namespace: webhook
spec:
  provider:
    aws:
      service: SecretsManager
      region: ap-northeast-1
      auth:
        jwt:
          serviceAccountRef:
            name: external-secrets-sa
---
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: webhook-secrets
  namespace: webhook
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: aws-secrets-manager
    kind: SecretStore
  target:
    name: webhook-secrets
  data:
    - secretKey: WEBHOOK_SECRET
      remoteRef:
        key: webhook/production/secret
    - secretKey: DATABASE_URL
      remoteRef:
        key: webhook/production/database-url
```

### Step 3: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤

#### 3.1 Kubernetes ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆä½œæˆ

**Namespaceä½œæˆ**:
```yaml
# namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: webhook
  labels:
    name: webhook
    security.sas-com.com/tier: critical
```

**ConfigMapä½œæˆ**:
```yaml
# configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: webhook-config
  namespace: webhook
data:
  ENVIRONMENT: "production"
  LOG_LEVEL: "info"
  ENABLE_METRICS: "true"
  ENABLE_HEALTH_CHECK: "true"
  RATE_LIMIT_GLOBAL: "1000"
  RATE_LIMIT_PER_IP: "60"
  RATE_LIMIT_PER_HOOK: "120"
  RATE_LIMIT_PER_REPO: "100"
  MAX_PAYLOAD_SIZE_MB: "10"
  IP_VALIDATION_STRICT: "true"
  GEO_RESTRICTIONS: "JP,US"
  ALLOWED_IPS: "140.82.112.0/20,143.55.64.0/20,185.199.108.0/22,192.30.252.0/22"
```

**Deploymentä½œæˆ (Node.js)**:
```yaml
# deployment-nodejs.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webhook-nodejs
  namespace: webhook
  labels:
    app: webhook-nodejs
    version: v1.0.0
    component: webhook-handler
    language: nodejs
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: webhook-nodejs
  template:
    metadata:
      labels:
        app: webhook-nodejs
        version: v1.0.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "3000"
        prometheus.io/path: "/metrics"
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        fsGroup: 1001
      containers:
      - name: webhook-nodejs
        image: sas-com/webhook-nodejs:v1.0.0
        imagePullPolicy: Always
        ports:
        - containerPort: 3000
          name: http
          protocol: TCP
        env:
        - name: HOST
          value: "0.0.0.0"
        - name: PORT
          value: "3000"
        envFrom:
        - configMapRef:
            name: webhook-config
        - secretRef:
            name: webhook-secrets
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          capabilities:
            drop:
              - ALL
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: logs
          mountPath: /app/logs
      volumes:
      - name: tmp
        emptyDir: {}
      - name: logs
        emptyDir: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - webhook-nodejs
              topologyKey: kubernetes.io/hostname
```

**Serviceä½œæˆ**:
```yaml
# service.yaml
apiVersion: v1
kind: Service
metadata:
  name: webhook-service
  namespace: webhook
  labels:
    app: webhook
    component: webhook-handler
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "3000"
    prometheus.io/path: "/metrics"
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 3000
    protocol: TCP
    name: http
  selector:
    app: webhook-nodejs
```

**Ingressä½œæˆ**:
```yaml
# ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: webhook-ingress
  namespace: webhook
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
    nginx.ingress.kubernetes.io/rate-limit: "60"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
spec:
  tls:
  - hosts:
    - webhook.sas-com.internal
    secretName: webhook-tls
  rules:
  - host: webhook.sas-com.internal
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: webhook-service
            port:
              number: 80
```

#### 3.2 ãƒ‡ãƒ—ãƒ­ã‚¤å®Ÿè¡Œ

```bash
# Namespaceä½œæˆ
kubectl apply -f namespace.yaml

# è¨­å®šé©ç”¨
kubectl apply -f configmap.yaml

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤
kubectl apply -f deployment-nodejs.yaml
kubectl apply -f service.yaml
kubectl apply -f ingress.yaml

# ãƒ‡ãƒ—ãƒ­ã‚¤çŠ¶æ³ç¢ºèª
kubectl get pods -n webhook
kubectl get svc -n webhook
kubectl get ingress -n webhook

# ãƒ­ã‚°ç¢ºèª
kubectl logs -n webhook -l app=webhook-nodejs -f

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
kubectl port-forward -n webhook svc/webhook-service 8080:80
curl http://localhost:8080/health
```

### Step 4: ç›£è¦–ãƒ»ãƒ­ã‚°ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### 4.1 Prometheus ServiceMonitor

```yaml
# servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: webhook-metrics
  namespace: webhook
  labels:
    app: webhook
spec:
  selector:
    matchLabels:
      app: webhook
  endpoints:
  - port: http
    path: /metrics
    interval: 15s
    scrapeTimeout: 10s
```

#### 4.2 Grafana Dashboard

```json
{
  "dashboard": {
    "title": "GitHub Webhook Security Dashboard",
    "tags": ["webhook", "github", "security"],
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(webhook_requests_total[5m]))",
            "legendFormat": "Requests/sec"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(webhook_errors_total[5m]))",
            "legendFormat": "Errors/sec"
          }
        ]
      },
      {
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, webhook_processing_duration_seconds)",
            "legendFormat": "95th percentile"
          }
        ]
      },
      {
        "title": "Security Events",
        "type": "table",
        "targets": [
          {
            "expr": "increase(webhook_security_events_total[1h])",
            "legendFormat": "{{event_type}}"
          }
        ]
      }
    ]
  }
}
```

#### 4.3 ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

```yaml
# alerts.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: webhook-alerts
  namespace: webhook
  labels:
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
  - name: webhook.rules
    rules:
    - alert: WebhookHighErrorRate
      expr: rate(webhook_errors_total[5m]) > 0.1
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "High webhook error rate detected"
        description: "Webhook error rate is {{ $value }} errors/sec"
        
    - alert: WebhookHighLatency
      expr: histogram_quantile(0.95, webhook_processing_duration_seconds) > 2
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High webhook processing latency"
        description: "95th percentile latency is {{ $value }}s"
        
    - alert: WebhookSecurityEvent
      expr: rate(webhook_security_events_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: "Security event detected"
        description: "Security event: {{ $labels.event_type }}"
        
    - alert: WebhookServiceDown
      expr: up{job="webhook"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Webhook service is down"
        description: "Webhook service {{ $labels.instance }} is down"
```

### Step 5: CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®š

#### 5.1 GitHub Actions ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```yaml
# .github/workflows/webhook-ci-cd.yml
name: Webhook CI/CD Pipeline

on:
  push:
    branches: [main, develop]
    paths: ['src/webhook-handlers/**']
  pull_request:
    branches: [main]
    paths: ['src/webhook-handlers/**']

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: sas-com/webhook

jobs:
  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³
  security-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'

  # ãƒ“ãƒ«ãƒ‰ãƒ»ãƒ†ã‚¹ãƒˆ
  build-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        language: [nodejs, python, go]
    steps:
    - uses: actions/checkout@v4
    
    - name: Build and Test - Node.js
      if: matrix.language == 'nodejs'
      run: |
        cd src/webhook-handlers/nodejs
        npm ci
        npm run build
        npm run test
        npm audit
    
    - name: Build and Test - Python
      if: matrix.language == 'python'
      run: |
        cd src/webhook-handlers/python
        pip install -r requirements.txt
        python -m pytest tests/ -v --cov=src
        python -m bandit -r src/
    
    - name: Build and Test - Go
      if: matrix.language == 'go'
      run: |
        cd src/webhook-handlers/go
        go mod verify
        go test -race -coverprofile=coverage.out ./...
        go run github.com/securecodewarrior/sast-scan/cmd/gosec@latest ./...
    
    - name: Build Docker Image
      run: |
        cd src/webhook-handlers/${{ matrix.language }}
        docker build -t ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}-${{ matrix.language }}:${{ github.sha }} .
        
    - name: Container Security Scan
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: '${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}-${{ matrix.language }}:${{ github.sha }}'
        format: 'table'
        exit-code: '1'
        severity: 'CRITICAL,HIGH'

  # ãƒ‡ãƒ—ãƒ­ã‚¤ (æœ¬ç•ª)
  deploy-production:
    if: github.ref == 'refs/heads/main'
    needs: [security-scan, build-test]
    runs-on: ubuntu-latest
    environment: production
    steps:
    - uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ap-northeast-1
    
    - name: Push to ECR
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        ECR_REPOSITORY: sas-webhook
      run: |
        aws ecr get-login-password --region ap-northeast-1 | docker login --username AWS --password-stdin $ECR_REGISTRY
        docker tag ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}-nodejs:${{ github.sha }} $ECR_REGISTRY/$ECR_REPOSITORY:nodejs-${{ github.sha }}
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:nodejs-${{ github.sha }}
    
    - name: Update Kubernetes
      run: |
        aws eks update-kubeconfig --region ap-northeast-1 --name sas-webhook-cluster
        kubectl set image deployment/webhook-nodejs webhook-nodejs=$ECR_REGISTRY/$ECR_REPOSITORY:nodejs-${{ github.sha }} -n webhook
        kubectl rollout status deployment/webhook-nodejs -n webhook
    
    - name: Verify Deployment
      run: |
        kubectl get pods -n webhook
        kubectl logs -n webhook -l app=webhook-nodejs --tail=100
        
        # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
        WEBHOOK_URL=$(kubectl get ingress -n webhook webhook-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        curl -f https://$WEBHOOK_URL/health || exit 1
```

## âš™ï¸ è¨­å®šç®¡ç†

### ç’°å¢ƒåˆ¥è¨­å®š

#### æœ¬ç•ªç’°å¢ƒè¨­å®š
```yaml
# config/production.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: webhook-config-prod
  namespace: webhook
data:
  ENVIRONMENT: "production"
  LOG_LEVEL: "info"
  ENABLE_METRICS: "true"
  ENABLE_HEALTH_CHECK: "true"
  ENABLE_CORS: "false"
  ENABLE_DOCS: "false"
  
  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
  IP_VALIDATION_STRICT: "true"
  ENABLE_SIGNATURE_VERIFY: "true"
  MAX_PAYLOAD_SIZE_MB: "10"
  
  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
  RATE_LIMIT_GLOBAL: "1000"
  RATE_LIMIT_PER_IP: "60"
  RATE_LIMIT_PER_HOOK: "120"
  RATE_LIMIT_PER_REPO: "100"
  
  # è¨±å¯IPç¯„å›²
  ALLOWED_IPS: |
    140.82.112.0/20,
    143.55.64.0/20,
    185.199.108.0/22,
    192.30.252.0/22,
    20.201.28.151/32,
    20.205.243.166/32
  
  # åœ°åŸŸåˆ¶é™
  GEO_RESTRICTIONS: "JP,US"
  
  # å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹
  ELASTICSEARCH_ENABLED: "true"
  REDIS_ENABLED: "true"
  PROMETHEUS_ENABLED: "true"
```

#### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
```yaml
# config/security-policies.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: webhook-network-policy
  namespace: webhook
spec:
  podSelector:
    matchLabels:
      app: webhook-nodejs
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 3000
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
  - to: []
    ports:
    - protocol: TCP
      port: 5432  # PostgreSQL
    - protocol: TCP
      port: 6379  # Redis
    - protocol: TCP
      port: 9200  # Elasticsearch
    - protocol: TCP
      port: 443   # HTTPS outbound
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: webhook-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
```

## ğŸ“Š ç›£è¦–ãƒ»ãƒ­ã‚°

### ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
```yaml
# é‡è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¸€è¦§
application_metrics:
  - name: webhook_requests_total
    type: counter
    description: "ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°"
    labels: [event_type, status, repository]
    
  - name: webhook_processing_duration_seconds
    type: histogram
    description: "å‡¦ç†æ™‚é–“"
    labels: [event_type, status]
    
  - name: webhook_errors_total
    type: counter
    description: "ã‚¨ãƒ©ãƒ¼ç·æ•°"
    labels: [error_type, event_type]
    
  - name: webhook_security_events_total
    type: counter
    description: "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆæ•°"
    labels: [event_type, severity]
    
  - name: webhook_active_connections
    type: gauge
    description: "ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ¥ç¶šæ•°"

# SLIãƒ¡ãƒˆãƒªã‚¯ã‚¹
sli_metrics:
  availability:
    target: 99.9%
    measurement: "up{job='webhook'}"
    
  latency:
    target: "95% < 500ms"
    measurement: "histogram_quantile(0.95, webhook_processing_duration_seconds)"
    
  error_rate:
    target: "< 1%"
    measurement: "rate(webhook_errors_total[5m]) / rate(webhook_requests_total[5m])"
    
  throughput:
    target: "1000 req/min"
    measurement: "rate(webhook_requests_total[1m]) * 60"
```

#### ã‚¤ãƒ³ãƒ•ãƒ© ãƒ¡ãƒˆãƒªã‚¯ã‚¹
```yaml
infrastructure_metrics:
  kubernetes:
    - kube_deployment_status_replicas_available
    - kube_deployment_status_replicas_unavailable
    - kube_pod_container_status_restarts_total
    - kube_pod_status_phase
    
  system:
    - node_cpu_seconds_total
    - node_memory_MemAvailable_bytes
    - node_filesystem_avail_bytes
    - node_network_receive_bytes_total
    
  application:
    - process_cpu_seconds_total
    - process_resident_memory_bytes
    - go_memstats_alloc_bytes
    - nodejs_heap_size_used_bytes
```

### ãƒ­ã‚°ç®¡ç†

#### ãƒ­ã‚°å½¢å¼æ¨™æº–åŒ–
```json
{
  "timestamp": "2025-09-10T21:00:00.000Z",
  "level": "info",
  "service": "webhook-nodejs",
  "version": "1.0.0",
  "environment": "production",
  "request_id": "req_1234567890",
  "delivery_id": "12345678-1234-1234-1234-123456789012",
  "event_type": "push",
  "message": "Webhook processed successfully",
  "fields": {
    "client_ip": "140.82.112.1",
    "repository": "sas-com/example-repo",
    "sender": "developer",
    "processing_time_ms": 150,
    "payload_size": 1024,
    "security_checks_passed": true,
    "sensitive_data_detected": false
  },
  "labels": {
    "team": "platform",
    "component": "webhook-handler",
    "criticality": "high"
  }
}
```

#### ELK Stackè¨­å®š
```yaml
# Elasticsearch Index Template
PUT _index_template/webhook-logs
{
  "index_patterns": ["webhook-logs-*"],
  "template": {
    "settings": {
      "number_of_shards": 3,
      "number_of_replicas": 1,
      "index.lifecycle.name": "webhook-logs-policy",
      "index.lifecycle.rollover_alias": "webhook-logs"
    },
    "mappings": {
      "properties": {
        "@timestamp": {"type": "date"},
        "level": {"type": "keyword"},
        "service": {"type": "keyword"},
        "request_id": {"type": "keyword"},
        "delivery_id": {"type": "keyword"},
        "event_type": {"type": "keyword"},
        "client_ip": {"type": "ip"},
        "repository": {"type": "keyword"},
        "processing_time_ms": {"type": "integer"},
        "security_checks_passed": {"type": "boolean"}
      }
    }
  }
}
```

### ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

#### é‡è¦åº¦åˆ¥ã‚¢ãƒ©ãƒ¼ãƒˆ
```yaml
# Critical (P0) - å³åº§å¯¾å¿œ
critical_alerts:
  - name: ServiceDown
    condition: up{job="webhook"} == 0
    duration: 1m
    notification: ["pagerduty", "slack-critical", "email"]
    
  - name: HighErrorRate
    condition: rate(webhook_errors_total[5m]) > 0.05
    duration: 2m
    notification: ["pagerduty", "slack-critical"]
    
  - name: SecurityBreach
    condition: rate(webhook_security_events_total{severity="critical"}[1m]) > 0
    duration: 0s
    notification: ["pagerduty", "slack-security", "siem"]

# High (P1) - 15åˆ†ä»¥å†…å¯¾å¿œ
high_alerts:
  - name: HighLatency
    condition: histogram_quantile(0.95, webhook_processing_duration_seconds) > 2
    duration: 5m
    notification: ["slack-ops", "email"]
    
  - name: MemoryUsageHigh
    condition: process_resident_memory_bytes / node_memory_MemTotal_bytes > 0.8
    duration: 10m
    notification: ["slack-ops"]

# Medium (P2) - 1æ™‚é–“ä»¥å†…å¯¾å¿œ
medium_alerts:
  - name: PodRestartFrequent
    condition: rate(kube_pod_container_status_restarts_total[1h]) > 0.1
    duration: 15m
    notification: ["slack-ops"]
    
  - name: DiskSpaceLow
    condition: node_filesystem_avail_bytes / node_filesystem_size_bytes < 0.2
    duration: 30m
    notification: ["slack-ops"]
```

## ğŸ”„ é‹ç”¨æ‰‹é †

### æ—¥æ¬¡é‹ç”¨

#### 1. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
```bash
#!/bin/bash
# daily-health-check.sh

echo "=== Webhook Service Health Check ==="
date

# Kubernetes PodçŠ¶æ…‹ç¢ºèª
kubectl get pods -n webhook
kubectl get svc -n webhook
kubectl get ingress -n webhook

# ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
WEBHOOK_URL="https://webhook.sas-com.internal"
echo "Health Check: $WEBHOOK_URL/health"
curl -f $WEBHOOK_URL/health | jq

echo "Metrics Check: $WEBHOOK_URL/metrics"
curl -s $WEBHOOK_URL/metrics | grep webhook_requests_total

# ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³
echo "=== Resource Usage ==="
kubectl top nodes
kubectl top pods -n webhook

# ãƒ­ã‚°ç¢ºèªï¼ˆç›´è¿‘1æ™‚é–“ã®ã‚¨ãƒ©ãƒ¼ï¼‰
echo "=== Recent Errors ==="
kubectl logs -n webhook -l app=webhook-nodejs --since=1h | grep ERROR | tail -10
```

#### 2. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»
```bash
#!/bin/bash
# security-audit.sh

echo "=== Security Audit ==="
date

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆç¢ºèª
curl -s "http://prometheus.monitoring.svc.cluster.local:9090/api/v1/query?query=increase(webhook_security_events_total[24h])" | jq

# ä¸å¯©ãªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œçŸ¥
curl -s "http://elasticsearch.logging.svc.cluster.local:9200/webhook-logs-*/_search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": {
      "bool": {
        "must": [
          {"range": {"@timestamp": {"gte": "now-24h"}}},
          {"term": {"level": "error"}},
          {"exists": {"field": "client_ip"}}
        ]
      }
    },
    "aggs": {
      "suspicious_ips": {
        "terms": {"field": "client_ip", "size": 10}
      }
    }
  }' | jq '.aggregations.suspicious_ips'

# è¨¼æ˜æ›¸æœ‰åŠ¹æœŸé™ç¢ºèª
echo "=== SSL Certificate Expiry ==="
openssl s_client -servername webhook.sas-com.internal -connect webhook.sas-com.internal:443 2>/dev/null | openssl x509 -noout -enddate
```

### é€±æ¬¡é‹ç”¨

#### 1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
```bash
#!/bin/bash
# weekly-performance-analysis.sh

echo "=== Weekly Performance Report ==="
date

# é€±é–“çµ±è¨ˆå–å¾—
WEEK_AGO=$(date -d "7 days ago" -u +"%Y-%m-%dT%H:%M:%SZ")
NOW=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆçµ±è¨ˆ
echo "Total Requests (7 days):"
curl -s "http://prometheus.monitoring.svc.cluster.local:9090/api/v1/query?query=increase(webhook_requests_total[7d])" | jq

# ã‚¨ãƒ©ãƒ¼ç‡
echo "Error Rate (7 days):"
curl -s "http://prometheus.monitoring.svc.cluster.local:9090/api/v1/query?query=rate(webhook_errors_total[7d]) / rate(webhook_requests_total[7d])" | jq

# ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“åˆ†æ
echo "95th Percentile Response Time:"
curl -s "http://prometheus.monitoring.svc.cluster.local:9090/api/v1/query?query=histogram_quantile(0.95, rate(webhook_processing_duration_seconds_bucket[7d]))" | jq

# å®¹é‡è¨ˆç”»
echo "=== Capacity Planning ==="
kubectl top nodes
kubectl describe node | grep -A 5 "Allocated resources"
```

#### 2. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ“ãƒ¥ãƒ¼
```bash
#!/bin/bash
# weekly-security-review.sh

echo "=== Weekly Security Review ==="
date

# è„†å¼±æ€§ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè¡Œ
trivy image sas-com/webhook-nodejs:latest --severity HIGH,CRITICAL

# ã‚³ãƒ³ãƒ†ãƒŠã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»
kubectl get pods -n webhook -o json | \
  jq '.items[] | select(.spec.securityContext.runAsNonRoot != true)'

# Network Policyç¢ºèª
kubectl get networkpolicy -n webhook -o yaml

# RBACç¢ºèª
kubectl auth can-i --list --as=system:serviceaccount:webhook:default
```

### æœˆæ¬¡é‹ç”¨

#### 1. ç½å®³å¾©æ—§ãƒ†ã‚¹ãƒˆ
```bash
#!/bin/bash
# monthly-dr-test.sh

echo "=== Disaster Recovery Test ==="
date

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç¢ºèª
echo "Database Backup Status:"
# å®Ÿéš›ã®ç’°å¢ƒã«å¿œã˜ã¦ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç¢ºèªã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè£…

# å¾©æ—§æ‰‹é †ãƒ†ã‚¹ãƒˆï¼ˆéæœ¬ç•ªç’°å¢ƒï¼‰
echo "Testing restore procedure..."
# å¾©æ—§æ‰‹é †ã®è‡ªå‹•ãƒ†ã‚¹ãƒˆå®Ÿè£…

# RTO/RPO ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¸¬å®š
echo "Recovery Metrics:"
echo "RTO Target: 1 hour"
echo "RPO Target: 15 minutes"
# å®Ÿéš›ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¸¬å®šãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…
```

#### 2. ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ãƒ¬ãƒ“ãƒ¥ãƒ¼
```bash
#!/bin/bash
# monthly-capacity-review.sh

echo "=== Monthly Capacity Review ==="
date

# æœˆé–“çµ±è¨ˆ
echo "Monthly Statistics:"
curl -s "http://prometheus.monitoring.svc.cluster.local:9090/api/v1/query?query=increase(webhook_requests_total[30d])" | jq

# ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨å‚¾å‘
echo "Resource Usage Trends:"
kubectl top nodes --sort-by=cpu
kubectl top pods -n webhook --sort-by=cpu

# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ¨å¥¨äº‹é …
echo "Scaling Recommendations:"
# å®Ÿéš›ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åˆ†æãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…
```

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ä¸€èˆ¬çš„ãªå•é¡Œã¨è§£æ±ºæ³•

#### 1. Webhookå—ä¿¡ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**: GitHubã‹ã‚‰ã®WebhookãŒå—ä¿¡ã•ã‚Œãªã„

**åŸå› ã¨è§£æ±ºæ³•**:

```bash
# DNSè§£æ±ºç¢ºèª
nslookup webhook.sas-com.internal

# SSLè¨¼æ˜æ›¸ç¢ºèª
openssl s_client -servername webhook.sas-com.internal -connect webhook.sas-com.internal:443

# Ingressç¢ºèª
kubectl get ingress -n webhook -o yaml
kubectl describe ingress webhook-ingress -n webhook

# Serviceç¢ºèª
kubectl get svc -n webhook
kubectl describe svc webhook-service -n webhook

# Podç¢ºèª
kubectl get pods -n webhook
kubectl describe pod webhook-nodejs-xxx -n webhook
kubectl logs webhook-nodejs-xxx -n webhook
```

#### 2. ç½²åæ¤œè¨¼ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**: `INVALID_SIGNATURE` ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ

**åŸå› ã¨è§£æ±ºæ³•**:

```bash
# Webhookç§˜å¯†éµç¢ºèª
kubectl get secret webhook-secrets -n webhook -o yaml | grep WEBHOOK_SECRET | base64 -d

# GitHubã®Webhookè¨­å®šç¢ºèª
# 1. GitHub > Settings > Webhooks
# 2. Secretå€¤ãŒä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
# 3. Content typeãŒ application/json ã«ãªã£ã¦ã„ã‚‹ã‹ç¢ºèª

# ãƒ­ã‚°ã§ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã¨ç½²åã‚’ç¢ºèª
kubectl logs -n webhook -l app=webhook-nodejs | grep signature
```

#### 3. Rate Limitè¶…é

**ç—‡çŠ¶**: `RATE_LIMIT_EXCEEDED` ã‚¨ãƒ©ãƒ¼ãŒé »ç™º

**åŸå› ã¨è§£æ±ºæ³•**:

```bash
# ç¾åœ¨ã®Rate Limitè¨­å®šç¢ºèª
kubectl get configmap webhook-config -n webhook -o yaml

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª
curl -s "http://prometheus.monitoring.svc.cluster.local:9090/api/v1/query?query=rate(webhook_requests_total[5m])"

# Rate Limitèª¿æ•´ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
kubectl patch configmap webhook-config -n webhook --type merge -p '{"data":{"RATE_LIMIT_PER_IP":"120"}}'
kubectl rollout restart deployment webhook-nodejs -n webhook
```

#### 4. ãƒ¡ãƒ¢ãƒªä¸è¶³

**ç—‡çŠ¶**: Pod ãŒ `OOMKilled` ã§ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã™ã‚‹

**åŸå› ã¨è§£æ±ºæ³•**:

```bash
# ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ç¢ºèª
kubectl top pods -n webhook --containers

# ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ç¢ºèª
kubectl describe pod webhook-nodejs-xxx -n webhook | grep -A 10 "Limits:"

# ãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚’èª¿æ•´
kubectl patch deployment webhook-nodejs -n webhook --type merge -p '{"spec":{"template":{"spec":{"containers":[{"name":"webhook-nodejs","resources":{"limits":{"memory":"8Gi"},"requests":{"memory":"2Gi"}}}]}}}}'
```

### ç·Šæ€¥å¯¾å¿œæ‰‹é †

#### 1. ç·Šæ€¥åœæ­¢æ‰‹é †

```bash
#!/bin/bash
# emergency-stop.sh

echo "=== EMERGENCY STOP PROCEDURE ==="
echo "Timestamp: $(date)"
echo "Operator: $USER"
read -p "Confirm emergency stop? (yes/no): " confirm

if [ "$confirm" = "yes" ]; then
    # ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯é®æ–­
    kubectl scale deployment webhook-nodejs -n webhook --replicas=0
    
    # Ingressç„¡åŠ¹åŒ–
    kubectl patch ingress webhook-ingress -n webhook --type json -p='[{"op": "replace", "path": "/spec/rules", "value": []}]'
    
    # é€šçŸ¥
    curl -X POST "$SLACK_WEBHOOK_URL" -d '{"text": "ğŸš¨ EMERGENCY STOP: Webhook service has been stopped"}'
    
    echo "Emergency stop completed"
else
    echo "Emergency stop cancelled"
fi
```

#### 2. ç·Šæ€¥å¾©æ—§æ‰‹é †

```bash
#!/bin/bash
# emergency-recovery.sh

echo "=== EMERGENCY RECOVERY PROCEDURE ==="
echo "Timestamp: $(date)"
echo "Operator: $USER"

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
kubectl get nodes
kubectl get pods -n webhook

# ã‚µãƒ¼ãƒ“ã‚¹å¾©æ—§
kubectl scale deployment webhook-nodejs -n webhook --replicas=3
kubectl rollout status deployment webhook-nodejs -n webhook

# Ingresså¾©æ—§
kubectl apply -f ingress.yaml

# ç–é€šç¢ºèª
sleep 30
curl -f https://webhook.sas-com.internal/health

# é€šçŸ¥
curl -X POST "$SLACK_WEBHOOK_URL" -d '{"text": "âœ… RECOVERY: Webhook service has been restored"}'

echo "Emergency recovery completed"
```

## ğŸ›¡ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é‹ç”¨

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»

#### 1. å®šæœŸã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³

```bash
#!/bin/bash
# security-scan.sh

echo "=== Security Scan ==="
date

# ã‚³ãƒ³ãƒ†ãƒŠè„†å¼±æ€§ã‚¹ã‚­ãƒ£ãƒ³
echo "Container Vulnerability Scan:"
trivy image sas-com/webhook-nodejs:latest --severity HIGH,CRITICAL --format json

# Kubernetesè¨­å®šã‚¹ã‚­ãƒ£ãƒ³
echo "Kubernetes Security Scan:"
kube-bench --targets node,policies,managedservices

# Network Policyç›£æŸ»
echo "Network Policy Audit:"
kubectl get networkpolicy -A -o yaml | grep -A 10 -B 5 "policyTypes"

# RBACç›£æŸ»
echo "RBAC Audit:"
kubectl auth can-i --list --as=system:serviceaccount:webhook:default

# Secretç›£æŸ»
echo "Secret Audit:"
kubectl get secrets -n webhook -o custom-columns="NAME:.metadata.name,TYPE:.type,DATA:.data"
```

#### 2. ã‚¢ã‚¯ã‚»ã‚¹ç›£æŸ»

```bash
#!/bin/bash
# access-audit.sh

echo "=== Access Audit ==="
date

# ç•°å¸¸ãªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
curl -s "http://elasticsearch.logging.svc.cluster.local:9200/webhook-logs-*/_search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": {
      "bool": {
        "must": [
          {"range": {"@timestamp": {"gte": "now-24h"}}},
          {"terms": {"level": ["error", "warn"]}}
        ]
      }
    },
    "aggs": {
      "top_error_ips": {
        "terms": {"field": "client_ip", "size": 20}
      },
      "error_types": {
        "terms": {"field": "fields.error_type", "size": 10}
      }
    }
  }'

# åœ°ç†çš„ã‚¢ã‚¯ã‚»ã‚¹åˆ†æ
echo "Geographic Access Analysis:"
# GeoIPåˆ†æå®Ÿè£…

# æ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
echo "Time Pattern Analysis:"
# ã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Ÿè£…
```

### ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œ

#### 1. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¤œçŸ¥

```bash
#!/bin/bash
# incident-detection.sh

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è„…å¨æ¤œçŸ¥
echo "=== Real-time Threat Detection ==="

# ç•°å¸¸ãªå¤§é‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
CURRENT_RATE=$(curl -s "http://prometheus.monitoring.svc.cluster.local:9090/api/v1/query?query=rate(webhook_requests_total[1m])" | jq '.data.result[0].value[1] | tonumber')

if (( $(echo "$CURRENT_RATE > 100" | bc -l) )); then
    echo "ğŸš¨ ALERT: Abnormally high request rate detected: $CURRENT_RATE req/s"
    # ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥å®Ÿè£…
fi

# ä¸å¯©ãªãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
SUSPICIOUS_PATTERNS=$(curl -s "http://elasticsearch.logging.svc.cluster.local:9200/webhook-logs-*/_search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": {
      "bool": {
        "must": [
          {"range": {"@timestamp": {"gte": "now-5m"}}},
          {"term": {"fields.sensitive_data_detected": true}}
        ]
      }
    }
  }' | jq '.hits.total.value')

if [ "$SUSPICIOUS_PATTERNS" -gt "0" ]; then
    echo "ğŸš¨ ALERT: Sensitive data patterns detected: $SUSPICIOUS_PATTERNS events"
    # ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œæ‰‹é †å®Ÿè¡Œ
fi
```

#### 2. è‡ªå‹•å¯¾å¿œã‚¢ã‚¯ã‚·ãƒ§ãƒ³

```bash
#!/bin/bash
# auto-response.sh

# IPè‡ªå‹•ãƒ–ãƒ­ãƒƒã‚¯æ©Ÿèƒ½
block_suspicious_ip() {
    local IP=$1
    echo "Blocking suspicious IP: $IP"
    
    # iptables ãƒ«ãƒ¼ãƒ«è¿½åŠ ï¼ˆå®Ÿéš›ã®ç’°å¢ƒã«å¿œã˜ã¦èª¿æ•´ï¼‰
    # kubectl exec -n kube-system iptables-pod -- iptables -A INPUT -s $IP -j DROP
    
    # ã¾ãŸã¯ Ingress Controller ãƒ¬ãƒ™ãƒ«ã§ãƒ–ãƒ­ãƒƒã‚¯
    kubectl patch configmap nginx-configuration -n ingress-nginx --type merge -p "{\"data\":{\"block-cidrs\":\"$IP\"}}"
}

# Rate Limitå‹•çš„èª¿æ•´
adjust_rate_limit() {
    local NEW_LIMIT=$1
    echo "Adjusting rate limit to: $NEW_LIMIT"
    
    kubectl patch configmap webhook-config -n webhook --type merge -p "{\"data\":{\"RATE_LIMIT_PER_IP\":\"$NEW_LIMIT\"}}"
    kubectl rollout restart deployment webhook-nodejs -n webhook
}

# ç·Šæ€¥ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ãƒ¢ãƒ¼ãƒ‰
emergency_maintenance_mode() {
    echo "Activating emergency maintenance mode"
    
    # ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ç”¨ã®Podã‚’ãƒ‡ãƒ—ãƒ­ã‚¤
    kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: maintenance-page
  namespace: webhook
spec:
  replicas: 1
  selector:
    matchLabels:
      app: maintenance-page
  template:
    metadata:
      labels:
        app: maintenance-page
    spec:
      containers:
      - name: maintenance
        image: nginx:alpine
        ports:
        - containerPort: 80
        volumeMounts:
        - name: maintenance-config
          mountPath: /etc/nginx/conf.d
      volumes:
      - name: maintenance-config
        configMap:
          name: maintenance-config
EOF

    # Ingress ã‚’ maintenance page ã«åˆ‡ã‚Šæ›¿ãˆ
    kubectl patch ingress webhook-ingress -n webhook --type json -p='[{"op": "replace", "path": "/spec/rules/0/http/paths/0/backend/service/name", "value": "maintenance-page"}]'
}
```

## ğŸ”„ ç½å®³å¾©æ—§

### RTO/RPOç›®æ¨™

| ç½å®³ãƒ¬ãƒ™ãƒ« | RTO (å¾©æ—§æ™‚é–“ç›®æ¨™) | RPO (ãƒ‡ãƒ¼ã‚¿æå¤±è¨±å®¹) | å¯¾å¿œæ‰‹é † |
|-----------|-------------------|---------------------|----------|
| **ãƒ¬ãƒ™ãƒ«1** | 5åˆ† | 0åˆ† | è‡ªå‹•ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ |
| **ãƒ¬ãƒ™ãƒ«2** | 30åˆ† | 5åˆ† | æ‰‹å‹•åˆ‡ã‚Šæ›¿ãˆ |
| **ãƒ¬ãƒ™ãƒ«3** | 4æ™‚é–“ | 30åˆ† | å®Œå…¨å¾©æ—§ |
| **ãƒ¬ãƒ™ãƒ«4** | 24æ™‚é–“ | 4æ™‚é–“ | å®Œå…¨å†æ§‹ç¯‰ |

### ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥

#### 1. ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

```yaml
# PostgreSQL ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
  namespace: webhook
spec:
  schedule: "0 2 * * *"  # æ¯æ—¥åˆå‰2æ™‚
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: postgres-backup
            image: postgres:13
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: password
            command:
            - /bin/bash
            - -c
            - |
              BACKUP_FILE="webhook-db-$(date +%Y%m%d-%H%M%S).sql"
              pg_dump -h postgres.webhook.svc.cluster.local -U webhook webhook > /backup/$BACKUP_FILE
              # S3/Azure Blob/GCS ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
              aws s3 cp /backup/$BACKUP_FILE s3://sas-webhook-backups/database/
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
          volumes:
          - name: backup-storage
            emptyDir: {}
          restartPolicy: OnFailure
```

#### 2. è¨­å®šãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

```bash
#!/bin/bash
# backup-config.sh

echo "=== Configuration Backup ==="
date

BACKUP_DIR="/backup/config-$(date +%Y%m%d-%H%M%S)"
mkdir -p $BACKUP_DIR

# Kubernetesè¨­å®šãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
kubectl get all -n webhook -o yaml > $BACKUP_DIR/webhook-resources.yaml
kubectl get configmap -n webhook -o yaml > $BACKUP_DIR/configmaps.yaml
kubectl get secret -n webhook -o yaml > $BACKUP_DIR/secrets.yaml
kubectl get ingress -n webhook -o yaml > $BACKUP_DIR/ingress.yaml
kubectl get networkpolicy -n webhook -o yaml > $BACKUP_DIR/network-policies.yaml

# Helm values ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
helm get values prometheus -n monitoring > $BACKUP_DIR/prometheus-values.yaml
helm get values grafana -n monitoring > $BACKUP_DIR/grafana-values.yaml

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®
tar -czf $BACKUP_DIR.tar.gz $BACKUP_DIR

# ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
aws s3 cp $BACKUP_DIR.tar.gz s3://sas-webhook-backups/config/

echo "Configuration backup completed: $BACKUP_DIR.tar.gz"
```

### å¾©æ—§æ‰‹é †

#### 1. å®Œå…¨ç½å®³å¾©æ—§

```bash
#!/bin/bash
# disaster-recovery.sh

echo "=== DISASTER RECOVERY PROCEDURE ==="
echo "Timestamp: $(date)"
echo "Operator: $USER"

# å‰ææ¡ä»¶ç¢ºèª
echo "Checking prerequisites..."
kubectl cluster-info
aws sts get-caller-identity

# 1. ã‚¤ãƒ³ãƒ•ãƒ©å¾©æ—§
echo "Step 1: Infrastructure Recovery"

# Kubernetes ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å¾©æ—§
if ! kubectl get nodes; then
    echo "Creating new Kubernetes cluster..."
    eksctl create cluster -f cluster-config.yaml
    aws eks update-kubeconfig --region ap-northeast-1 --name sas-webhook-cluster
fi

# å¿…é ˆã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå¾©æ—§
echo "Installing essential components..."
helm install nginx-ingress ingress-nginx/ingress-nginx --namespace ingress-nginx --create-namespace
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml

# 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©æ—§
echo "Step 2: Database Recovery"

# æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å–å¾—
LATEST_BACKUP=$(aws s3 ls s3://sas-webhook-backups/database/ | sort | tail -n 1 | awk '{print $4}')
echo "Restoring from backup: $LATEST_BACKUP"

# PostgreSQLå¾©æ—§
kubectl apply -f postgres-deployment.yaml
kubectl wait --for=condition=ready pod -l app=postgres -n webhook --timeout=300s

# ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ
kubectl exec -n webhook postgres-0 -- psql -U webhook -c "DROP DATABASE IF EXISTS webhook;"
kubectl exec -n webhook postgres-0 -- psql -U webhook -c "CREATE DATABASE webhook;"
aws s3 cp s3://sas-webhook-backups/database/$LATEST_BACKUP /tmp/
kubectl cp /tmp/$LATEST_BACKUP webhook/postgres-0:/tmp/
kubectl exec -n webhook postgres-0 -- psql -U webhook webhook < /tmp/$LATEST_BACKUP

# 3. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å¾©æ—§
echo "Step 3: Application Recovery"

# è¨­å®šå¾©æ—§
LATEST_CONFIG=$(aws s3 ls s3://sas-webhook-backups/config/ | sort | tail -n 1 | awk '{print $4}')
aws s3 cp s3://sas-webhook-backups/config/$LATEST_CONFIG /tmp/
tar -xzf /tmp/$LATEST_CONFIG -C /tmp/

# Kubernetes ãƒªã‚½ãƒ¼ã‚¹å¾©æ—§
kubectl apply -f /tmp/config-*/webhook-resources.yaml
kubectl apply -f /tmp/config-*/configmaps.yaml
kubectl apply -f /tmp/config-*/secrets.yaml
kubectl apply -f /tmp/config-*/ingress.yaml

# 4. ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ å¾©æ—§
echo "Step 4: Monitoring Recovery"

helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --values /tmp/config-*/prometheus-values.yaml

helm install grafana grafana/grafana \
  --namespace monitoring \
  --values /tmp/config-*/grafana-values.yaml

# 5. ç–é€šç¢ºèª
echo "Step 5: Verification"

# Podç¢ºèª
kubectl get pods -n webhook
kubectl wait --for=condition=ready pod -l app=webhook-nodejs -n webhook --timeout=300s

# ã‚µãƒ¼ãƒ“ã‚¹ç¢ºèª
sleep 60
WEBHOOK_URL="https://webhook.sas-com.internal"
if curl -f $WEBHOOK_URL/health; then
    echo "âœ… Health check passed"
else
    echo "âŒ Health check failed"
    exit 1
fi

# 6. é€šçŸ¥
echo "Step 6: Notification"

curl -X POST "$SLACK_WEBHOOK_URL" -d "{\"text\": \"âœ… DISASTER RECOVERY COMPLETED\\nTimestamp: $(date)\\nOperator: $USER\\nRTO: $(date -d \"$START_TIME\" '+%H:%M:%S')\"}"

echo "=== DISASTER RECOVERY COMPLETED ==="
```

#### 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼åˆ‡ã‚Šæ›¿ãˆ

```bash
#!/bin/bash
# datacenter-failover.sh

echo "=== DATACENTER FAILOVER ==="

# ãƒ—ãƒ©ã‚¤ãƒãƒªDCåœæ­¢ç¢ºèª
PRIMARY_DC="ap-northeast-1"
SECONDARY_DC="ap-southeast-1"

echo "Checking primary DC: $PRIMARY_DC"
if ! aws ec2 describe-availability-zones --region $PRIMARY_DC 2>/dev/null; then
    echo "Primary DC is unavailable, initiating failover to $SECONDARY_DC"
    
    # DNSåˆ‡ã‚Šæ›¿ãˆ
    aws route53 change-resource-record-sets --hosted-zone-id Z123456789 --change-batch '{
        "Changes": [{
            "Action": "UPSERT",
            "ResourceRecordSet": {
                "Name": "webhook.sas-com.internal",
                "Type": "CNAME",
                "TTL": 60,
                "ResourceRecords": [{"Value": "webhook-secondary.sas-com.internal"}]
            }
        }]
    }'
    
    # ã‚»ã‚«ãƒ³ãƒ€ãƒªDCã§ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹
    aws eks update-kubeconfig --region $SECONDARY_DC --name sas-webhook-cluster-secondary
    kubectl scale deployment webhook-nodejs -n webhook --replicas=3
    
    echo "Failover completed to secondary DC: $SECONDARY_DC"
else
    echo "Primary DC is available, no failover needed"
fi
```

---

## ğŸ“ ç·Šæ€¥é€£çµ¡å…ˆ

### ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä½“åˆ¶

| ãƒ¬ãƒ™ãƒ« | å¯¾è±¡ | é€£çµ¡å…ˆ | å¯¾å¿œæ™‚é–“ |
|--------|------|--------|----------|
| **L1 (Critical)** | å³åº§ | github@sas-com.com<br>090-xxxx-xxxx (ã‚ªãƒ³ã‚³ãƒ¼ãƒ«) | 24/7 |
| **L2 (High)** | 1æ™‚é–“ä»¥å†… | github@sas-com.com | å¹³æ—¥ 9-18æ™‚ |
| **L3 (Medium)** | 4æ™‚é–“ä»¥å†… | github@sas-com.com | å¹³æ—¥ 9-18æ™‚ |
| **L4 (Low)** | ç¿Œå–¶æ¥­æ—¥ | github@sas-com.com | å¹³æ—¥ 9-18æ™‚ |

### å¤–éƒ¨ãƒ™ãƒ³ãƒ€ãƒ¼é€£çµ¡å…ˆ

- **AWS Support**: Enterprise ã‚µãƒãƒ¼ãƒˆ
- **GitHub Support**: Premium ã‚µãƒãƒ¼ãƒˆ
- **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ™ãƒ³ãƒ€ãƒ¼**: SOC 24/7å¯¾å¿œ

---

**æ›´æ–°å±¥æ­´**:
- 2025-09-10: åˆç‰ˆä½œæˆ
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ“ãƒ¥ãƒ¼: æ‰¿èªå¾…ã¡
- æ¬¡å›æ›´æ–°äºˆå®š: 2025-12-10

**é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**:
- [GITHUB_WEBHOOK_SECURITY_GUIDE.md](./GITHUB_WEBHOOK_SECURITY_GUIDE.md)
- [WEBHOOK_API_SPECIFICATION.md](./WEBHOOK_API_SPECIFICATION.md)

**æ‹…å½“è€…**:
- é‹ç”¨è¨­è¨ˆ: GitHubç®¡ç†ãƒãƒ¼ãƒ 
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ“ãƒ¥ãƒ¼: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒ¼ãƒ 
- æ‰¿èª: CTO Office